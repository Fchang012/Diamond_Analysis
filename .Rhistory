nnName <- paste("./Neural_Net_Model/nnModel", i, sep = "")
assign(paste("nn", i, sep = ""), readRDS(nnName), envir = .GlobalEnv)
#Choose which NN to look at
nodeNumber <- 3
nn <- get(paste("nn", nodeNumber, sep = ""))
#View NN
plot(nn, rep="best")
#Predict using ANN Train
pr.nn_train <- compute(nn, train_NN[,1:38])
require(caret)
#Choose which NN to look at
nodeNumber <- 3
nn <- get(paste("nn", nodeNumber, sep = ""))
#View NN
plot(nn, rep="best")
#Predict using ANN Train
pr.nn_train <- compute(nn, train_NN[,1:38])
require(neuralnet)
#Choose which NN to look at
nodeNumber <- 3
nn <- get(paste("nn", nodeNumber, sep = ""))
#View NN
plot(nn, rep="best")
#Predict using ANN Train
pr.nn_train <- compute(nn, train_NN[,1:38])
#Extract results for train_NN
pr.nn_train <- round(pr.nn_train$net.result)
#original values
original_Values <- train_NN$Hospital.overall.rating
#Fixing imbalance issue
u <- sort(union(original_Values, pr.nn_train))
#Confusion Matrix
CM <- table(factor(pr.nn_train,u), factor(original_Values,u))
print("Confusion Matrix Train")
confusionMatrix(pr.nn_train, original_Values)
#incorrect classification
print(paste("Overall Wrong Train: ", 1-sum(diag(CM))/sum(CM)) )
#Predict using ANN Test
pr.nn_test <- compute(nn, test_NN[,1:38])
#Extract results for test_NN
pr.nn_test <- round(pr.nn_test$net.result)
#original values
original_Values <- test_NN$Hospital.overall.rating
#Fixing imbalance issue
u <- sort(union(original_Values, pr.nn_test))
#Confusion Matrix
CM <- table(factor(pr.nn_test, u), factor(original_Values,u))
print("Confusion Matrix Test")
confusionMatrix(pr.nn_test, original_Values)
#incorrect classification
print(paste("Overall Wrong Test: ", 1-sum(diag(CM))/sum(CM)) )
View(test)
#Import libraries
# library(rattle)
library(RColorBrewer)
# library(RGtk2)
require(PerformanceAnalytics)
require(dplyr)
require(neuralnet)
require(ggplot2)
require(caret)
require(foreach)
require(doParallel)
MedIncomeDF <- read.csv("Combined_Median_Income_2015_Education_Demographic.csv", sep=";")
#Remove non-necessary columns
drops <- c("FIPStxt", "State", "Area.name", "Median_Household_Income_2015")
MedIncomeDF <- MedIncomeDF[, !(names(MedIncomeDF) %in% drops)]
#Scaling for comparisons between different methods
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
#  [2] "Unemployment_rate_2015"
MedIncomeDF$Unemployment_rate_2015 <- scl(MedIncomeDF$Unemployment_rate_2015)
#  [3] "Med_HH_Income_Percent_of_State_Total_2015"
MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015 <- scl(MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015)
#  [4] "Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015"
MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015)
#  [5] "Percent.of.adults.with.a.high.school.diploma.only.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015)
#  [6] "Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015"
MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015)
#  [7] "Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015)
#  [9] "TOT_POP"
MedIncomeDF$TOT_POP <- scl(MedIncomeDF$TOT_POP)
# [10] "P_MALE"
MedIncomeDF$P_MALE <- scl(MedIncomeDF$P_MALE)
# [11] "P_FEMALE"
MedIncomeDF$P_FEMALE <- scl(MedIncomeDF$P_FEMALE)
#75% for sample size
smp_size <- floor(0.75 * nrow(MedIncomeDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(MedIncomeDF)), size = smp_size)
train <- MedIncomeDF[train_ind, ]
test <- MedIncomeDF[-train_ind, ]
#Note: Med_HH_Income_Percent_of_State_Total_2015 is that countys median income / state median income
#Correlations
cor <- cor(subset(MedIncomeDF, select = -c(index, Above.Median)), y=MedIncomeDF$Above.Median)
cor
# png('./Diagrams/MedianIncomCorr.png', width = 4096, height = 2160)
# chart.Correlation(subset(MedIncomeDF[,2:12]))
# dev.off()
View(MedIncomeDF)
names(MedIncomeDF)
cor(MedIncomeDF)
chart.Correlation(subset(MedIncomeDF[,2:12]))
?chart.Correlation
Correlation(subset(MedIncomeDF[,2:12]))
#Import libraries
# library(rattle)
library(RColorBrewer)
# library(RGtk2)
require(PerformanceAnalytics)
require(dplyr)
require(neuralnet)
require(ggplot2)
require(caret)
require(foreach)
require(doParallel)
MedIncomeDF <- read.csv("Combined_Median_Income_2015_Education_Demographic.csv", sep=";")
#Remove non-necessary columns
drops <- c("FIPStxt", "State", "Area.name", "Median_Household_Income_2015")
MedIncomeDF <- MedIncomeDF[, !(names(MedIncomeDF) %in% drops)]
#Scaling for comparisons between different methods
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
#  [2] "Unemployment_rate_2015"
MedIncomeDF$Unemployment_rate_2015 <- scl(MedIncomeDF$Unemployment_rate_2015)
#  [3] "Med_HH_Income_Percent_of_State_Total_2015"
MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015 <- scl(MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015)
#  [4] "Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015"
MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015)
#  [5] "Percent.of.adults.with.a.high.school.diploma.only.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015)
#  [6] "Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015"
MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015)
#  [7] "Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015)
#  [9] "TOT_POP"
MedIncomeDF$TOT_POP <- scl(MedIncomeDF$TOT_POP)
# [10] "P_MALE"
MedIncomeDF$P_MALE <- scl(MedIncomeDF$P_MALE)
# [11] "P_FEMALE"
MedIncomeDF$P_FEMALE <- scl(MedIncomeDF$P_FEMALE)
#75% for sample size
smp_size <- floor(0.75 * nrow(MedIncomeDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(MedIncomeDF)), size = smp_size)
train <- MedIncomeDF[train_ind, ]
test <- MedIncomeDF[-train_ind, ]
#Note: Med_HH_Income_Percent_of_State_Total_2015 is that countys median income / state median income
#Correlations
cor <- cor(subset(MedIncomeDF, select = -c(index, Above.Median)), y=MedIncomeDF$Above.Median)
cor
# png('./Diagrams/MedianIncomCorr.png', width = 4096, height = 2160)
# chart.Correlation(subset(MedIncomeDF[,2:12]))
# dev.off()
#Import libraries
# library(rattle)
library(RColorBrewer)
# library(RGtk2)
require(PerformanceAnalytics)
require(dplyr)
require(neuralnet)
require(ggplot2)
require(caret)
require(foreach)
require(doParallel)
MedIncomeDF <- read.csv("Combined_Median_Income_2015_Education_Demographic.csv", sep=";")
#Remove non-necessary columns
drops <- c("FIPStxt", "State", "Area.name", "Median_Household_Income_2015")
MedIncomeDF <- MedIncomeDF[, !(names(MedIncomeDF) %in% drops)]
#Scaling for comparisons between different methods
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
#  [2] "Unemployment_rate_2015"
MedIncomeDF$Unemployment_rate_2015 <- scl(MedIncomeDF$Unemployment_rate_2015)
#  [3] "Med_HH_Income_Percent_of_State_Total_2015"
MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015 <- scl(MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015)
#  [4] "Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015"
MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015)
#  [5] "Percent.of.adults.with.a.high.school.diploma.only.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015)
#  [6] "Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015"
MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015)
#  [7] "Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015)
#  [9] "TOT_POP"
MedIncomeDF$TOT_POP <- scl(MedIncomeDF$TOT_POP)
# [10] "P_MALE"
MedIncomeDF$P_MALE <- scl(MedIncomeDF$P_MALE)
# [11] "P_FEMALE"
MedIncomeDF$P_FEMALE <- scl(MedIncomeDF$P_FEMALE)
#75% for sample size
smp_size <- floor(0.75 * nrow(MedIncomeDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(MedIncomeDF)), size = smp_size)
train <- MedIncomeDF[train_ind, ]
test <- MedIncomeDF[-train_ind, ]
#Note: Med_HH_Income_Percent_of_State_Total_2015 is that countys median income / state median income
#Correlations
cor <- cor(subset(MedIncomeDF, select = -c(index, Above.Median)), y=MedIncomeDF$Above.Median)
cor
# png('./Diagrams/MedianIncomCorr.png', width = 4096, height = 2160)
# chart.Correlation(subset(MedIncomeDF[,2:12]))
# dev.off()
#Libraries
library(rpart)
library(rpart.plot)
library(caret)
require(foreach)
require(doParallel)
#Break data
train_Tree <- subset(train, select = -c(index))
test_Tree <- subset(test, select = -c(index))
#Set up formula
n <- names(train_Tree)
f <- as.formula(paste("Above.Median ~", paste(n[!n %in% "Above.Median"], collapse = " + ")))
fit_tree <- rpart(f,
data=train,
method="class",
control = rpart.control(minsplit = 1, cp = 0.001)
)
PredictionTrain <- predict(fit_tree, train, type = "class")
ResultTrain <- data.frame(County.State = train$index, Predict = Prediction, Above.Median = train$Above.Median)
PredictionTrain <- predict(fit_tree, train, type = "class")
ResultTrain <- data.frame(County.State = train$index, Predict = PredictionTrain, Above.Median = train$Above.Median)
PredictionTest <- predict(fit_tree, test, type = "class")
ResultTest <- data.frame(County.State = test$index, Predict = Prediction, Above.Median = test$Above.Median)
PredictionTrain <- predict(fit_tree, train, type = "class")
ResultTrain <- data.frame(County.State = train$index, Predict = PredictionTrain, Above.Median = train$Above.Median)
PredictionTest <- predict(fit_tree, test, type = "class")
ResultTest <- data.frame(County.State = test$index, Predict = PredictionTest, Above.Median = test$Above.Median)
#Confusion Matrix Print out Train
print("Confusion Matrix Train")
confusionMatrix(PredictionTrain, train$Above.Median)
#Confusion Matrix
CM <- table(PredictionTrain, train$Above.Median)
#Accuracy
print(paste("Overall Wrong Train: ", 1-sum(diag(CM))/sum(CM)) )
#Confusion Matrix Print out Test
print("Confusion Matrix Test")
confusionMatrix(PredictionTest, test$Above.Median)
#Confusion Matrix
CM <- table(PredictionTest, test$Above.Median)
#Accuracy
print(paste("Overall Wrong Test: ", 1-sum(diag(CM))/sum(CM)) )
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
getNodeSet(top, '//a')
test <- getNodeSet(top, '//a')
names(test)
test[1]
require(XML)
require(RCurl)
require(rlist)
require(rvest)
# Get cur dir from source of R Script
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
# Bluenile Table
BlueNileTable <- read_xml("BlueNileTable.html", as_html = T)
BlueNileTable <- xmlTreeParse(BlueNileTable)[[1]]
top <- xmlRoot(BlueNileTable)
test <- getNodeSet(top, '//a')
require(XML)
require(RCurl)
require(rlist)
require(rvest)
# Get cur dir from source of R Script
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
# Bluenile Table
BlueNileTable <- read_xml("BlueNileTable.html", as_html = T)
BlueNileTable <- xmlTreeParse(BlueNileTable)[[1]]
top <- xmlRoot(BlueNileTable)
test <- getNodeSet(top, '//a')
tet
test[[1]]
test[1]
test1 <- getNodeSet(test1, '//span')
test1 <- getNodeSet(test, '//span')
test1 <- getNodeSet(top, '//span')
names(test1)
test1
test1 <- getNodeSet(top, '//span[class="single-cell]')
test1 <- getNodeSet(top, '//span[@class="single-cell]')
test1 <- getNodeSet(top, '//span[@class="single-cell"]')
test1
test
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
eachRecord[[1]]
xmlToDataFrame(eachRecord[[1]])
xmlToDataFrame(nodes = eachRecord)
xmlToDataFrame(nodes = top)
blueNileDataframe <- xmlToDataFrame(nodes = eachRecord)
View(blueNileDataframe)
getNodeSet(top, '//div[@class="row"')
getNodeSet(top, '//div[@class="row"]')
headers <- getNodeSet(top, '//div[@class="row"]')
headers
xmlValue(headers)
headers[[1]]
headers[[1]][1]
headers[[1]][2]
headers[[1]][3]
xmlValue(headers[[1]][3])
xmlAttrs(headers)
xmlAttrs(headers[[1]])
xmlAttrs(headers[[1]][1])
xmlAttrs(headers[[1]][2])
xmlAttrs(headers[[1]][3])
xmlAttrs(headers[[2]])
headers
headers[[1]]
xmlAttrs(headers[[1]])
headers[[1]]
headers[[1]][3]
headers[[1]][3]$div
xmlValue(headers[[1]][3]$div)
headers[[1]]
length(headers[[1]])
headers[[1]][1]
xpathSApply(headers[[1]],xmlAttrs)
xpathSApply(headers[[1]], "/div/div", xmlAttrs)
xpathSApply(headers[[1]], "/div", xmlAttrs)
xpathSApply(headers, "/div", xmlAttrs)
xpathSApply(headers[[1]], "/div", xmlAttrs)
xpathSApply(headers[[1]], "/div", xmlValue])
xpathSApply(headers[[1]], "/div", xmlValue)
xpathSApply(headers[1], "/div", xmlValue)
headers[[1]]
xmlValue(headers[[1]])
xmlValue(headers[1])
xmlValue(headers1)
xmlValue(headers[[1]])
?xmlValue
xmlTextNode(headers[[1]])
xmlTextNode(headers[[1]])
xmlValue(xmlTextNode(headers[[1]]))
xmlAttrs(headers[[1]])
xmlValue(headers[[1]])
test <- xmlValue(headers[[1]])
headers[[1]]
getNodeSet(headers[[1]])
getNodeSet(headers[[1]],"//div")
getNodeSet(headers[[1]],"//div")[2]
xml_length(headers[[1]])
headers[[1]]
xmlValue(headers[[1]][1])
xmlValue(headers[[1]][1])
headers[[1]][1]
headers <- getNodeSet(headers, '//div')
headers <- getNodeSet(headers[[1]], '//div')
headers
headers
headers[1]
headers <- headers[-1]
headers
headers
test <- list()
headers
headers[1]
headers[[1]]
headerList
headers[[1]]
xmlValue(headers[[1]])
test.append("1")
headers[[1]]
xmlValue(headerspp1)
xmlValue(headers[[1]])
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
headers
xmlValue[headerList]
xmlValue(headerList)
xmlValue(headerList[1])
unlist(headerList)
unlist(header)
unlist(headers)
unlist(headers[[1]])
unlist(headers[[1]])[1]
unlist(headers[[1]])[2]
unlist(headers[[1]])[3]
unlist(headers[[1]])[4]
unlist(headers[[1]])[5]
headers
headers[1]
xmlValue(headers[1])
xmlValue(unlist(headers[1]))
length(headers)
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
for(i in length(headers)){
list.append(headerList, xmlValue(headers[[i]]))
}
for(i in length(headers)){
list.append(headerList, xmlValue(headers[[i]]))
}
warnings
warnings()
headers[[1]]
xmlValue(headers[[1]])
list.append(headerList,xmlValue(headers[[1]]))
for(i in length(headers)){
headerList <- list.append(headerList, xmlValue(headers[[i]]))
}
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
headers
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
for(i in length(headers)){
headerList[i] <- xmlValue(headers[[i]])
}
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
for(i in 1:length(headers)){
headerList[i] <- xmlValue(headers[[i]])
}
View(blueNileDataframe)
names(blueNileDataframe) <- headerList
View(blueNileDataframe)
View(blueNileDataframe)
headers
eachRecord
View(blueNileDataframe)
xmlValue(eachRecord)
xmlValue(eachRecord[[1]])
xmlAttrs(eachRecord)
xmlAttrs(eachRecord[[1]])
xmlName(eachRecord)
xmlName(eachRecord[[1]])
xmlName(eachRecord[[2]])
xmlName(eachRecord[[3]])
eachRecord
eachRecord[[1]]
eachRecord[[1]]
getNodeSet(eachRecord[[1]])
getNodeSet(eachRecord[[1]], '//a[@href]')
xmlGetAttr(reachRecord, 'href')
xmlGetAttr(eachRecord, 'href')
xmlGetAttr(eachRecord[[1]], 'href')
length(eachRecord)
xpathSApply(eachRecord,'//a', xmlGetAttr, 'href')
xmlRoot(eachRecord)
xmlRoot(eachRecord[1])
xmlRoot(eachRecord[[1]])
xmlTreeParse(eachRecord)
eachRecord
eachRecord[[1]]
xmlTreeParse(eachRecord)
eachRecord$div
eachRecord$a
eachRecord[[1]][1]
eachRecord[[1]]
getNodeSet(eachRecord)
getNodeSet
?getNodeSet
xpathSApply(top, '//a', xmlGetAttr, 'href')
test <- xpathSApply(top, '//a', xmlGetAttr, 'href')
urlList <- list()
for(i in 1:length(eachRecord)){
urlList[i] <- xmlGetAttr(eachRecord[[i]], 'href')
}
urlList <- as.data.frame(urlList)
View(urlList)
urlList <- t(as.data.frame(urlList))
View(urlList)
urlList <- list()
for(i in 1:length(eachRecord)){
urlList[i] <- xmlGetAttr(eachRecord[[i]], 'href')
}
urlList <- t(as.data.frame(urlList))
View(urlList)
cbind(blueNileDataframe, urlList)
test <- cbind(blueNileDataframe, urlList)
View(test)
?t
?cbind
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
View(test)
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
View(test)
test[]
clear
test[1]
test[[1]]
test <- as.tibble(test)
install.packages("tibble")
require(tibble)
test <- as.tibble(test)
View(test)
test[1,]
test[1,]$urlList
test1 <- test[1,]
View(test1)
test1 <- test[-1,]
test1$urlList
test1 <- test[220,]
test1
test1$urlList
View(test1)
View(test1)
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
source('~/workspace/Fchang012/Diamond_Analysis/HTML_Table_Parse.R', echo=TRUE)
View(blueNileDataframe)
