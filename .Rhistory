# [11] "Mortality.national.comparison"
#Remove Not Available
HospitalDF <- HospitalDF[!HospitalDF$Mortality.national.comparison ==  levels(HospitalDF$Mortality.national.comparison)[3],]
# HospitalDF$Mortality.national.comparison <- dynName("Mortality.national.comparison", HospitalDF$Mortality.national.comparison)
# #Debugging
# #Translate back
# HospitalDF$Mortality.national.comparison <- translateBack(Mortality.national.comparison.dict, HospitalDF$Mortality.national.comparison)
# [12] "Safety.of.care.national.comparison"
#Remove Not Available
HospitalDF <- HospitalDF[!HospitalDF$Safety.of.care.national.comparison == levels(HospitalDF$Safety.of.care.national.comparison)[3],]
# HospitalDF$Safety.of.care.national.comparison <- dynName("Safety.of.care.national.comparison", HospitalDF$Safety.of.care.national.comparison)
# #Debugging
# #Translate back
# HospitalDF$Safety.of.care.national.comparison <- translateBack(Safety.of.care.national.comparison.dict, HospitalDF$Safety.of.care.national.comparison)
# [13] "Readmission.national.comparison"
# HospitalDF$Readmission.national.comparison <- dynName("Readmission.national.comparison", HospitalDF$Readmission.national.comparison)
# #Debugging
# #Translate back
# HospitalDF$Readmission.national.comparison <- translateBack(Readmission.national.comparison.dict, HospitalDF$Readmission.national.comparison)
# [14] "Patient.experience.national.comparison"
#Remove Not Available
HospitalDF <- HospitalDF[!HospitalDF$Patient.experience.national.comparison == levels(HospitalDF$Patient.experience.national.comparison)[3],]
# HospitalDF$Patient.experience.national.comparison <- dynName("Patient.experience.national.comparison", HospitalDF$Patient.experience.national.comparison)
# #Debugging
# #Translate back
# HospitalDF$Patient.experience.national.comparison <- translateBack(Patient.experience.national.comparison.dict, HospitalDF$Patient.experience.national.comparison)
# [15] "Effectiveness.of.care.national.comparison"
#Remove Not Available
HospitalDF <- HospitalDF[!HospitalDF$Effectiveness.of.care.national.comparison == levels(HospitalDF$Effectiveness.of.care.national.comparison)[3],]
# HospitalDF$Effectiveness.of.care.national.comparison <- dynName("Effectiveness.of.care.national.comparison", HospitalDF$Effectiveness.of.care.national.comparison)
# #Debugging
# #Translate back
# HospitalDF$Effectiveness.of.care.national.comparison <- translateBack(Effectiveness.of.care.national.comparison.dict, HospitalDF$Effectiveness.of.care.national.comparison)
# [16] "Timeliness.of.care.national.comparison"
#Remove Not Available
HospitalDF <- HospitalDF[!HospitalDF$Timeliness.of.care.national.comparison == levels(HospitalDF$Timeliness.of.care.national.comparison)[3], ]
# HospitalDF$Timeliness.of.care.national.comparison <- dynName("Timeliness.of.care.national.comparison", HospitalDF$Timeliness.of.care.national.comparison)
# #Debugging
# #Translate back
# HospitalDF$Timeliness.of.care.national.comparison <- translateBack(Timeliness.of.care.national.comparison.dict, HospitalDF$Timeliness.of.care.national.comparison)
# [17] "Efficient.use.of.medical.imaging.national.comparison"
#Remove Not Available
HospitalDF <- HospitalDF[!HospitalDF$Efficient.use.of.medical.imaging.national.comparison == levels(HospitalDF$Efficient.use.of.medical.imaging.national.comparison)[3], ]
# HospitalDF$Efficient.use.of.medical.imaging.national.comparison <- dynName("Efficient.use.of.medical.imaging.national.comparison", HospitalDF$Efficient.use.of.medical.imaging.national.comparison)
# #Debugging
# #Translate back
# HospitalDF$Efficient.use.of.medical.imaging.national.comparison <- translateBack(Efficient.use.of.medical.imaging.national.comparison.dict, HospitalDF$Efficient.use.of.medical.imaging.national.comparison)
# # #Debugging Comparisons
# debugIndex <- sample(1:length(HospitalDF$Provider.ID), 5, replace=T)
# x <- HospitalDF[debugIndex,]
#
# y <- HospitalDF_Check[HospitalDF_Check$Provider.ID %in% x$Provider.ID, ]
HospitalDF <- HospitalDF[c(1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 8)]
#Clean
HospitalDF <- droplevels(HospitalDF)
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
HospitalDF$Median.Income <- scl(HospitalDF$Median.Income)
#75% for sample size
smp_size <- floor(0.75 * nrow(HospitalDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(HospitalDF)), size = smp_size)
train <- HospitalDF[train_ind, ]
test <- HospitalDF[-train_ind, ]
#Setting sit
set.seed(123)
#Get datasets
train_NN <- data.frame(subset(train, select = -c(Provider.ID)))
test_NN <- data.frame(subset(test, select = -c(Provider.ID)))
#dummy variable testing
train_NN <- data.frame(predict(dummyVars("~ .", data=train_NN), newdata = train_NN))
require(caret)
set.seed(123)
#Get datasets
train_NN <- data.frame(subset(train, select = -c(Provider.ID)))
test_NN <- data.frame(subset(test, select = -c(Provider.ID)))
#dummy variable testing
train_NN <- data.frame(predict(dummyVars("~ .", data=train_NN), newdata = train_NN))
test_NN <- data.frame(predict(dummyVars("~ .", data=test_NN), newdata = test_NN))
#Load Nerualnets
#Load function
dynName <- function(i){
nnName <- paste("./Neural_Net_Model/nnModel", i, sep = "")
#Make dictionaries for the column
assign(paste("nn", i, sep = ""), readRDS(nnName), envir = .GlobalEnv)
}
for(i in 2:39){
dynName(i)
}
i=3
dynName(i)
#Choose which NN to look at
nodeNumber <- 3
nn <- get(paste("nn", nodeNumber, sep = ""))
#View NN
plot(nn, rep="best")
#Predict using ANN Train
pr.nn_train <- compute(nn, train_NN[,1:38])
assign(paste("nn", i, sep = ""), readRDS(nnName), envir = .GlobalEnv)
nnName <- paste("./Neural_Net_Model/nnModel", i, sep = "")
assign(paste("nn", i, sep = ""), readRDS(nnName), envir = .GlobalEnv)
#Choose which NN to look at
nodeNumber <- 3
nn <- get(paste("nn", nodeNumber, sep = ""))
#View NN
plot(nn, rep="best")
#Predict using ANN Train
pr.nn_train <- compute(nn, train_NN[,1:38])
require(caret)
#Choose which NN to look at
nodeNumber <- 3
nn <- get(paste("nn", nodeNumber, sep = ""))
#View NN
plot(nn, rep="best")
#Predict using ANN Train
pr.nn_train <- compute(nn, train_NN[,1:38])
require(neuralnet)
#Choose which NN to look at
nodeNumber <- 3
nn <- get(paste("nn", nodeNumber, sep = ""))
#View NN
plot(nn, rep="best")
#Predict using ANN Train
pr.nn_train <- compute(nn, train_NN[,1:38])
#Extract results for train_NN
pr.nn_train <- round(pr.nn_train$net.result)
#original values
original_Values <- train_NN$Hospital.overall.rating
#Fixing imbalance issue
u <- sort(union(original_Values, pr.nn_train))
#Confusion Matrix
CM <- table(factor(pr.nn_train,u), factor(original_Values,u))
print("Confusion Matrix Train")
confusionMatrix(pr.nn_train, original_Values)
#incorrect classification
print(paste("Overall Wrong Train: ", 1-sum(diag(CM))/sum(CM)) )
#Predict using ANN Test
pr.nn_test <- compute(nn, test_NN[,1:38])
#Extract results for test_NN
pr.nn_test <- round(pr.nn_test$net.result)
#original values
original_Values <- test_NN$Hospital.overall.rating
#Fixing imbalance issue
u <- sort(union(original_Values, pr.nn_test))
#Confusion Matrix
CM <- table(factor(pr.nn_test, u), factor(original_Values,u))
print("Confusion Matrix Test")
confusionMatrix(pr.nn_test, original_Values)
#incorrect classification
print(paste("Overall Wrong Test: ", 1-sum(diag(CM))/sum(CM)) )
View(test)
#Import libraries
# library(rattle)
library(RColorBrewer)
# library(RGtk2)
require(PerformanceAnalytics)
require(dplyr)
require(neuralnet)
require(ggplot2)
require(caret)
require(foreach)
require(doParallel)
MedIncomeDF <- read.csv("Combined_Median_Income_2015_Education_Demographic.csv", sep=";")
#Remove non-necessary columns
drops <- c("FIPStxt", "State", "Area.name", "Median_Household_Income_2015")
MedIncomeDF <- MedIncomeDF[, !(names(MedIncomeDF) %in% drops)]
#Scaling for comparisons between different methods
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
#  [2] "Unemployment_rate_2015"
MedIncomeDF$Unemployment_rate_2015 <- scl(MedIncomeDF$Unemployment_rate_2015)
#  [3] "Med_HH_Income_Percent_of_State_Total_2015"
MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015 <- scl(MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015)
#  [4] "Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015"
MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015)
#  [5] "Percent.of.adults.with.a.high.school.diploma.only.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015)
#  [6] "Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015"
MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015)
#  [7] "Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015)
#  [9] "TOT_POP"
MedIncomeDF$TOT_POP <- scl(MedIncomeDF$TOT_POP)
# [10] "P_MALE"
MedIncomeDF$P_MALE <- scl(MedIncomeDF$P_MALE)
# [11] "P_FEMALE"
MedIncomeDF$P_FEMALE <- scl(MedIncomeDF$P_FEMALE)
#75% for sample size
smp_size <- floor(0.75 * nrow(MedIncomeDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(MedIncomeDF)), size = smp_size)
train <- MedIncomeDF[train_ind, ]
test <- MedIncomeDF[-train_ind, ]
#Note: Med_HH_Income_Percent_of_State_Total_2015 is that countys median income / state median income
#Correlations
cor <- cor(subset(MedIncomeDF, select = -c(index, Above.Median)), y=MedIncomeDF$Above.Median)
cor
# png('./Diagrams/MedianIncomCorr.png', width = 4096, height = 2160)
# chart.Correlation(subset(MedIncomeDF[,2:12]))
# dev.off()
View(MedIncomeDF)
names(MedIncomeDF)
cor(MedIncomeDF)
chart.Correlation(subset(MedIncomeDF[,2:12]))
?chart.Correlation
Correlation(subset(MedIncomeDF[,2:12]))
#Import libraries
# library(rattle)
library(RColorBrewer)
# library(RGtk2)
require(PerformanceAnalytics)
require(dplyr)
require(neuralnet)
require(ggplot2)
require(caret)
require(foreach)
require(doParallel)
MedIncomeDF <- read.csv("Combined_Median_Income_2015_Education_Demographic.csv", sep=";")
#Remove non-necessary columns
drops <- c("FIPStxt", "State", "Area.name", "Median_Household_Income_2015")
MedIncomeDF <- MedIncomeDF[, !(names(MedIncomeDF) %in% drops)]
#Scaling for comparisons between different methods
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
#  [2] "Unemployment_rate_2015"
MedIncomeDF$Unemployment_rate_2015 <- scl(MedIncomeDF$Unemployment_rate_2015)
#  [3] "Med_HH_Income_Percent_of_State_Total_2015"
MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015 <- scl(MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015)
#  [4] "Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015"
MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015)
#  [5] "Percent.of.adults.with.a.high.school.diploma.only.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015)
#  [6] "Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015"
MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015)
#  [7] "Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015)
#  [9] "TOT_POP"
MedIncomeDF$TOT_POP <- scl(MedIncomeDF$TOT_POP)
# [10] "P_MALE"
MedIncomeDF$P_MALE <- scl(MedIncomeDF$P_MALE)
# [11] "P_FEMALE"
MedIncomeDF$P_FEMALE <- scl(MedIncomeDF$P_FEMALE)
#75% for sample size
smp_size <- floor(0.75 * nrow(MedIncomeDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(MedIncomeDF)), size = smp_size)
train <- MedIncomeDF[train_ind, ]
test <- MedIncomeDF[-train_ind, ]
#Note: Med_HH_Income_Percent_of_State_Total_2015 is that countys median income / state median income
#Correlations
cor <- cor(subset(MedIncomeDF, select = -c(index, Above.Median)), y=MedIncomeDF$Above.Median)
cor
# png('./Diagrams/MedianIncomCorr.png', width = 4096, height = 2160)
# chart.Correlation(subset(MedIncomeDF[,2:12]))
# dev.off()
#Import libraries
# library(rattle)
library(RColorBrewer)
# library(RGtk2)
require(PerformanceAnalytics)
require(dplyr)
require(neuralnet)
require(ggplot2)
require(caret)
require(foreach)
require(doParallel)
MedIncomeDF <- read.csv("Combined_Median_Income_2015_Education_Demographic.csv", sep=";")
#Remove non-necessary columns
drops <- c("FIPStxt", "State", "Area.name", "Median_Household_Income_2015")
MedIncomeDF <- MedIncomeDF[, !(names(MedIncomeDF) %in% drops)]
#Scaling for comparisons between different methods
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
#  [2] "Unemployment_rate_2015"
MedIncomeDF$Unemployment_rate_2015 <- scl(MedIncomeDF$Unemployment_rate_2015)
#  [3] "Med_HH_Income_Percent_of_State_Total_2015"
MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015 <- scl(MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015)
#  [4] "Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015"
MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015)
#  [5] "Percent.of.adults.with.a.high.school.diploma.only.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015)
#  [6] "Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015"
MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015)
#  [7] "Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015)
#  [9] "TOT_POP"
MedIncomeDF$TOT_POP <- scl(MedIncomeDF$TOT_POP)
# [10] "P_MALE"
MedIncomeDF$P_MALE <- scl(MedIncomeDF$P_MALE)
# [11] "P_FEMALE"
MedIncomeDF$P_FEMALE <- scl(MedIncomeDF$P_FEMALE)
#75% for sample size
smp_size <- floor(0.75 * nrow(MedIncomeDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(MedIncomeDF)), size = smp_size)
train <- MedIncomeDF[train_ind, ]
test <- MedIncomeDF[-train_ind, ]
#Note: Med_HH_Income_Percent_of_State_Total_2015 is that countys median income / state median income
#Correlations
cor <- cor(subset(MedIncomeDF, select = -c(index, Above.Median)), y=MedIncomeDF$Above.Median)
cor
# png('./Diagrams/MedianIncomCorr.png', width = 4096, height = 2160)
# chart.Correlation(subset(MedIncomeDF[,2:12]))
# dev.off()
#Libraries
library(rpart)
library(rpart.plot)
library(caret)
require(foreach)
require(doParallel)
#Break data
train_Tree <- subset(train, select = -c(index))
test_Tree <- subset(test, select = -c(index))
#Set up formula
n <- names(train_Tree)
f <- as.formula(paste("Above.Median ~", paste(n[!n %in% "Above.Median"], collapse = " + ")))
fit_tree <- rpart(f,
data=train,
method="class",
control = rpart.control(minsplit = 1, cp = 0.001)
)
PredictionTrain <- predict(fit_tree, train, type = "class")
ResultTrain <- data.frame(County.State = train$index, Predict = Prediction, Above.Median = train$Above.Median)
PredictionTrain <- predict(fit_tree, train, type = "class")
ResultTrain <- data.frame(County.State = train$index, Predict = PredictionTrain, Above.Median = train$Above.Median)
PredictionTest <- predict(fit_tree, test, type = "class")
ResultTest <- data.frame(County.State = test$index, Predict = Prediction, Above.Median = test$Above.Median)
PredictionTrain <- predict(fit_tree, train, type = "class")
ResultTrain <- data.frame(County.State = train$index, Predict = PredictionTrain, Above.Median = train$Above.Median)
PredictionTest <- predict(fit_tree, test, type = "class")
ResultTest <- data.frame(County.State = test$index, Predict = PredictionTest, Above.Median = test$Above.Median)
#Confusion Matrix Print out Train
print("Confusion Matrix Train")
confusionMatrix(PredictionTrain, train$Above.Median)
#Confusion Matrix
CM <- table(PredictionTrain, train$Above.Median)
#Accuracy
print(paste("Overall Wrong Train: ", 1-sum(diag(CM))/sum(CM)) )
#Confusion Matrix Print out Test
print("Confusion Matrix Test")
confusionMatrix(PredictionTest, test$Above.Median)
#Confusion Matrix
CM <- table(PredictionTest, test$Above.Median)
#Accuracy
print(paste("Overall Wrong Test: ", 1-sum(diag(CM))/sum(CM)) )
require(ggplot2)
require(corrplot)
require(tibble)
# Some basic analysis on diamond prices based on https://github.com/amarder/diamonds/blob/master/diamonds.Rmd
# Get cur dir from source of R Script
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
# Import Data
FinalDF <- read.csv("./Clean_Data/Diamond_Data.csv", sep = rawToChar(as.raw(127)))
# Reorder Factor Levels
FinalDF$Cut <- factor(FinalDF$Cut, levels(FinalDF$Cut)[c(1,2,4,5,3)])
FinalDF$Clarity <- factor(FinalDF$Clarity, levels(FinalDF$Clarity)[c(1,4,5,2,3)])
## Plot of Caret vs Price and Amarder Analysis --------------------------------------------------------
# Plots
# Cut
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Cut)) +
geom_point(aes(shape=Cut, color=Cut)) +
ggtitle("Caret vs Price With Cut As Legend") +
theme_bw()
# Color
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Color)) +
geom_point(aes(shape=Color, color=Color)) +
ggtitle("Caret vs Price With Color As Legend") +
theme_bw()
# Clarity
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Clarity)) +
geom_point(aes(shape=Clarity, color=Clarity)) +
ggtitle("Caret vs Price With Clarity As Legend") +
theme_bw()
## Blue Nile’s buying guide describes how the four C’s (cut, color, clarity, and carat weight) are the most
## important characteristics when buying a diamond. It seems reasonable to model price as a function of those
## four characteristics. Having played around with the data bit, a multiplicative model seems like a good choice.
## I model price as a product of carat weight raised to the power β times multipliers for the cut, color, and
## clarity of the diamond.
##
## Pricei∝caratβi ⋅ cuti ⋅ colori ⋅ clarityi
##
## Taking log’s of both sides allows this model to be estimated using a linear regression
##
## log(pricei)=α+βlog(carati)+δcuti+δcolori+δclarityi+ϵi
# Create dummy var for Cut, Color, Clarity and disregard the CutGood, ColorG, and ClarityVS2 as they are dependent on the other respective variables
tempDF <- as.tibble(cbind(model.matrix( ~ Cut - 1, data=FinalDF), model.matrix( ~ Color - 1, data=FinalDF), model.matrix( ~ Clarity - 1, data=FinalDF)))
FinalDF <- as.tibble(cbind(FinalDF, tempDF))
colnames(FinalDF) <- make.names(colnames(FinalDF))
fString <- paste('log(Price) ~ log(Carat)+', paste(colnames(FinalDF)[-c(1:6, 11, 15, 20)], collapse = '+'), sep = '')
fit <- lm(fString, data=FinalDF)
# Correlation
linDependTerm <- alias(fit)
# Find the coeff of fit and use it to plot fitted line
# https://www.statmethods.net/stats/regression.html for more documentation on Fitting lm
coeff=coefficients(fit)
# PLotting regression line
ggplot(FinalDF, aes(x=Carat,
y=Price,
color=Cut)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
# Adding in the regression forecasts back into df
FinalDF <- cbind(FinalDF, Forecast=exp(predict(fit)))
FinalDF <- cbind(FinalDF, Residual=resid(fit))
source('~/workspace/Fchang012/Diamond_Analysis/Diamond_Analysis.R')
View(FinalDF)
quantile(FinalDF$Residual, 0.01)
focus <- FinalDF[FinalDF$Residual <= quantile(FinalDF$Residual, 0.01), ]
View(focus)
View(focus)
focus <- FinalDF[FinalDF$Residual <= quantile(FinalDF$Residual, 0.01), ]
#
ggplot(focus,
aes(x=Carat,
y=Price,
color=Cut)) +
geom_point(aes(shape=Cut, color=Cut)) +
ggtitle("Top 1%") +
theme_bw()
ggplot(focus,
aes(x=Carat,
y=Price,
color=Cut)) +
geom_point(aes(shape=Cut, color=Cut)) +
ggtitle("Top 1%") +
stat_smooth(method = "lm", col = "red") +
theme_bw()
ggplot(focus,
aes(x=Carat,
y=Price,
color=Cut)) +
geom_point(aes(shape=Cut, color=Cut)) +
ggtitle("Top 1%") +
theme_bw()
View(focus)
focus[focus$CutAstor.Ideal == 1,]$urlList
focus[focus$CutAstor.Ideal == 1,]
ggplot(focus,
aes(x=Carat,
y=Price,
color=Color)) +
geom_point(aes(shape=Color, color=Color)) +
ggtitle("Top 1% Color") +
theme_bw()
ggplot(focus,
aes(x=Carat,
y=Price,
color=Clarity)) +
geom_point(aes(shape=Clarity, color=Clarity)) +
ggtitle("Top 1% Clarity") +
theme_bw()
factors(focus$Carat)
levels(focus$Cut)
levels(focus$Color)
levels(focus$Clarity)
focus[focus$Clarity == "VVS1",]
focus <- FinalDF[FinalDF$Residual <= quantile(FinalDF$Residual, 0.1), ]
# Plot of Top 1%
ggplot(focus,
aes(x=Carat,
y=Price,
color=Cut)) +
geom_point(aes(shape=Cut, color=Cut)) +
ggtitle("Top 1% Cut") +
theme_bw()
ggplot(focus,
aes(x=Carat,
y=Price,
color=Color)) +
geom_point(aes(shape=Color, color=Color)) +
ggtitle("Top 1% Color") +
theme_bw()
ggplot(focus,
aes(x=Carat,
y=Price,
color=Clarity)) +
geom_point(aes(shape=Clarity, color=Clarity)) +
ggtitle("Top 1% Clarity") +
theme_bw()
View(focus)
focus[focus$Cut >= "Ideal",]
focus[focus$Cut == "Ideal",]
focus[focus$Cut == "Ideal" && focus$Carat >=1,]
focus[focus$Carat >=1,]
focus[focus$Cut == "Ideal",]
library(dplyr)
focus %>% filter(1 <= carat, carat < 2, cut >= 'Ideal', color >= 'H', clarity >= 'VS1')
focus %>% filter(1 <= carat, Carat < 2, Cut >= 'Ideal', Color >= 'H', Clarity >= 'VS1')
focus %>% filter(1 <= Carat, Carat < 2, Cut >= 'Ideal', Color >= 'H', Clarity >= 'VS1')
focus %>% filter(1 <= Carat, Carat < 2)
