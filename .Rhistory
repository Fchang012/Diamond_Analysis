plot(nn, rep="best")
#Predict using ANN Train
pr.nn_train <- compute(nn, train_NN[,1:38])
require(neuralnet)
#Choose which NN to look at
nodeNumber <- 3
nn <- get(paste("nn", nodeNumber, sep = ""))
#View NN
plot(nn, rep="best")
#Predict using ANN Train
pr.nn_train <- compute(nn, train_NN[,1:38])
#Extract results for train_NN
pr.nn_train <- round(pr.nn_train$net.result)
#original values
original_Values <- train_NN$Hospital.overall.rating
#Fixing imbalance issue
u <- sort(union(original_Values, pr.nn_train))
#Confusion Matrix
CM <- table(factor(pr.nn_train,u), factor(original_Values,u))
print("Confusion Matrix Train")
confusionMatrix(pr.nn_train, original_Values)
#incorrect classification
print(paste("Overall Wrong Train: ", 1-sum(diag(CM))/sum(CM)) )
#Predict using ANN Test
pr.nn_test <- compute(nn, test_NN[,1:38])
#Extract results for test_NN
pr.nn_test <- round(pr.nn_test$net.result)
#original values
original_Values <- test_NN$Hospital.overall.rating
#Fixing imbalance issue
u <- sort(union(original_Values, pr.nn_test))
#Confusion Matrix
CM <- table(factor(pr.nn_test, u), factor(original_Values,u))
print("Confusion Matrix Test")
confusionMatrix(pr.nn_test, original_Values)
#incorrect classification
print(paste("Overall Wrong Test: ", 1-sum(diag(CM))/sum(CM)) )
View(test)
#Import libraries
# library(rattle)
library(RColorBrewer)
# library(RGtk2)
require(PerformanceAnalytics)
require(dplyr)
require(neuralnet)
require(ggplot2)
require(caret)
require(foreach)
require(doParallel)
MedIncomeDF <- read.csv("Combined_Median_Income_2015_Education_Demographic.csv", sep=";")
#Remove non-necessary columns
drops <- c("FIPStxt", "State", "Area.name", "Median_Household_Income_2015")
MedIncomeDF <- MedIncomeDF[, !(names(MedIncomeDF) %in% drops)]
#Scaling for comparisons between different methods
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
#  [2] "Unemployment_rate_2015"
MedIncomeDF$Unemployment_rate_2015 <- scl(MedIncomeDF$Unemployment_rate_2015)
#  [3] "Med_HH_Income_Percent_of_State_Total_2015"
MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015 <- scl(MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015)
#  [4] "Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015"
MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015)
#  [5] "Percent.of.adults.with.a.high.school.diploma.only.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015)
#  [6] "Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015"
MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015)
#  [7] "Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015)
#  [9] "TOT_POP"
MedIncomeDF$TOT_POP <- scl(MedIncomeDF$TOT_POP)
# [10] "P_MALE"
MedIncomeDF$P_MALE <- scl(MedIncomeDF$P_MALE)
# [11] "P_FEMALE"
MedIncomeDF$P_FEMALE <- scl(MedIncomeDF$P_FEMALE)
#75% for sample size
smp_size <- floor(0.75 * nrow(MedIncomeDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(MedIncomeDF)), size = smp_size)
train <- MedIncomeDF[train_ind, ]
test <- MedIncomeDF[-train_ind, ]
#Note: Med_HH_Income_Percent_of_State_Total_2015 is that countys median income / state median income
#Correlations
cor <- cor(subset(MedIncomeDF, select = -c(index, Above.Median)), y=MedIncomeDF$Above.Median)
cor
# png('./Diagrams/MedianIncomCorr.png', width = 4096, height = 2160)
# chart.Correlation(subset(MedIncomeDF[,2:12]))
# dev.off()
View(MedIncomeDF)
names(MedIncomeDF)
cor(MedIncomeDF)
chart.Correlation(subset(MedIncomeDF[,2:12]))
?chart.Correlation
Correlation(subset(MedIncomeDF[,2:12]))
#Import libraries
# library(rattle)
library(RColorBrewer)
# library(RGtk2)
require(PerformanceAnalytics)
require(dplyr)
require(neuralnet)
require(ggplot2)
require(caret)
require(foreach)
require(doParallel)
MedIncomeDF <- read.csv("Combined_Median_Income_2015_Education_Demographic.csv", sep=";")
#Remove non-necessary columns
drops <- c("FIPStxt", "State", "Area.name", "Median_Household_Income_2015")
MedIncomeDF <- MedIncomeDF[, !(names(MedIncomeDF) %in% drops)]
#Scaling for comparisons between different methods
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
#  [2] "Unemployment_rate_2015"
MedIncomeDF$Unemployment_rate_2015 <- scl(MedIncomeDF$Unemployment_rate_2015)
#  [3] "Med_HH_Income_Percent_of_State_Total_2015"
MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015 <- scl(MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015)
#  [4] "Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015"
MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015)
#  [5] "Percent.of.adults.with.a.high.school.diploma.only.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015)
#  [6] "Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015"
MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015)
#  [7] "Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015)
#  [9] "TOT_POP"
MedIncomeDF$TOT_POP <- scl(MedIncomeDF$TOT_POP)
# [10] "P_MALE"
MedIncomeDF$P_MALE <- scl(MedIncomeDF$P_MALE)
# [11] "P_FEMALE"
MedIncomeDF$P_FEMALE <- scl(MedIncomeDF$P_FEMALE)
#75% for sample size
smp_size <- floor(0.75 * nrow(MedIncomeDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(MedIncomeDF)), size = smp_size)
train <- MedIncomeDF[train_ind, ]
test <- MedIncomeDF[-train_ind, ]
#Note: Med_HH_Income_Percent_of_State_Total_2015 is that countys median income / state median income
#Correlations
cor <- cor(subset(MedIncomeDF, select = -c(index, Above.Median)), y=MedIncomeDF$Above.Median)
cor
# png('./Diagrams/MedianIncomCorr.png', width = 4096, height = 2160)
# chart.Correlation(subset(MedIncomeDF[,2:12]))
# dev.off()
#Import libraries
# library(rattle)
library(RColorBrewer)
# library(RGtk2)
require(PerformanceAnalytics)
require(dplyr)
require(neuralnet)
require(ggplot2)
require(caret)
require(foreach)
require(doParallel)
MedIncomeDF <- read.csv("Combined_Median_Income_2015_Education_Demographic.csv", sep=";")
#Remove non-necessary columns
drops <- c("FIPStxt", "State", "Area.name", "Median_Household_Income_2015")
MedIncomeDF <- MedIncomeDF[, !(names(MedIncomeDF) %in% drops)]
#Scaling for comparisons between different methods
#Scale the data between 0-1
scl <- function(x){
(x - min(x)) / (max(x) - min(x))
}
#  [2] "Unemployment_rate_2015"
MedIncomeDF$Unemployment_rate_2015 <- scl(MedIncomeDF$Unemployment_rate_2015)
#  [3] "Med_HH_Income_Percent_of_State_Total_2015"
MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015 <- scl(MedIncomeDF$Med_HH_Income_Percent_of_State_Total_2015)
#  [4] "Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015"
MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.less.than.a.high.school.diploma.2011.2015)
#  [5] "Percent.of.adults.with.a.high.school.diploma.only.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.high.school.diploma.only.2011.2015)
#  [6] "Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015"
MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.completing.some.college.or.associate.s.degree.2011.2015)
#  [7] "Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015"
MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015 <- scl(MedIncomeDF$Percent.of.adults.with.a.bachelor.s.degree.or.higher.2011.2015)
#  [9] "TOT_POP"
MedIncomeDF$TOT_POP <- scl(MedIncomeDF$TOT_POP)
# [10] "P_MALE"
MedIncomeDF$P_MALE <- scl(MedIncomeDF$P_MALE)
# [11] "P_FEMALE"
MedIncomeDF$P_FEMALE <- scl(MedIncomeDF$P_FEMALE)
#75% for sample size
smp_size <- floor(0.75 * nrow(MedIncomeDF))
#Set the seed to make reproduceable
set.seed(123)
train_ind <- sample(seq_len(nrow(MedIncomeDF)), size = smp_size)
train <- MedIncomeDF[train_ind, ]
test <- MedIncomeDF[-train_ind, ]
#Note: Med_HH_Income_Percent_of_State_Total_2015 is that countys median income / state median income
#Correlations
cor <- cor(subset(MedIncomeDF, select = -c(index, Above.Median)), y=MedIncomeDF$Above.Median)
cor
# png('./Diagrams/MedianIncomCorr.png', width = 4096, height = 2160)
# chart.Correlation(subset(MedIncomeDF[,2:12]))
# dev.off()
#Libraries
library(rpart)
library(rpart.plot)
library(caret)
require(foreach)
require(doParallel)
#Break data
train_Tree <- subset(train, select = -c(index))
test_Tree <- subset(test, select = -c(index))
#Set up formula
n <- names(train_Tree)
f <- as.formula(paste("Above.Median ~", paste(n[!n %in% "Above.Median"], collapse = " + ")))
fit_tree <- rpart(f,
data=train,
method="class",
control = rpart.control(minsplit = 1, cp = 0.001)
)
PredictionTrain <- predict(fit_tree, train, type = "class")
ResultTrain <- data.frame(County.State = train$index, Predict = Prediction, Above.Median = train$Above.Median)
PredictionTrain <- predict(fit_tree, train, type = "class")
ResultTrain <- data.frame(County.State = train$index, Predict = PredictionTrain, Above.Median = train$Above.Median)
PredictionTest <- predict(fit_tree, test, type = "class")
ResultTest <- data.frame(County.State = test$index, Predict = Prediction, Above.Median = test$Above.Median)
PredictionTrain <- predict(fit_tree, train, type = "class")
ResultTrain <- data.frame(County.State = train$index, Predict = PredictionTrain, Above.Median = train$Above.Median)
PredictionTest <- predict(fit_tree, test, type = "class")
ResultTest <- data.frame(County.State = test$index, Predict = PredictionTest, Above.Median = test$Above.Median)
#Confusion Matrix Print out Train
print("Confusion Matrix Train")
confusionMatrix(PredictionTrain, train$Above.Median)
#Confusion Matrix
CM <- table(PredictionTrain, train$Above.Median)
#Accuracy
print(paste("Overall Wrong Train: ", 1-sum(diag(CM))/sum(CM)) )
#Confusion Matrix Print out Test
print("Confusion Matrix Test")
confusionMatrix(PredictionTest, test$Above.Median)
#Confusion Matrix
CM <- table(PredictionTest, test$Above.Median)
#Accuracy
print(paste("Overall Wrong Test: ", 1-sum(diag(CM))/sum(CM)) )
source('~/workspace/Fchang012/Diamond_Analysis/Diamond_Analysis.R')
coeff
# PLotting
ggplot(FinalDF, aes(x=Carat,
y=Price)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
fir
fit
summary(fit)
# PLotting
ggplot(FinalDF, aes(x=Carat,
y=Price,
color=cut)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
ggplot(FinalDF, aes(x=Carat,
y=Price,
color=Cut)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
source('~/workspace/Fchang012/Diamond_Analysis/Diamond_Analysis.R')
ggplotRegression(fit)
fit$model
fit$model[2]
ggplotRegression(fit)
names(fit$model)
debugSource('~/workspace/Fchang012/Diamond_Analysis/Diamond_Analysis.R')
debugSource('~/workspace/Fchang012/Diamond_Analysis/Diamond_Analysis.R')
debugSource('~/workspace/Fchang012/Diamond_Analysis/Diamond_Analysis.R')
debugSource('~/workspace/Fchang012/Diamond_Analysis/Diamond_Analysis.R')
debugSource('~/workspace/Fchang012/Diamond_Analysis/Diamond_Analysis.R')
source('~/workspace/Fchang012/Diamond_Analysis/Diamond_Analysis.R')
ggplot(FinalDF, aes(x=Carat,
y=Price)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
ggplot(FinalDF, aes(x=Carat,
y=Price,
color=cut)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
ggplot(FinalDF, aes(x=Carat,
y=Price,
color=Cut)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
View(FinalDF)
predict(fit)
exp(1)
predict(fit)
exp(predict(fit))
Test <- cbind(FinalDF, exp(predict(fit)))
View(Test)
Test <- cbind(FinalDF, Forecast=exp(predict(fit)))
View(Test)
resid(fit)
require(ggplot2)
require(corrplot)
require(tibble)
# Some basic analysis on diamond prices based on https://github.com/amarder/diamonds/blob/master/diamonds.Rmd
# Get cur dir from source of R Script
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
# Import Data
FinalDF <- read.csv("./Clean_Data/Diamond_Data.csv", sep = rawToChar(as.raw(127)))
# Reorder Factor Levels
FinalDF$Cut <- factor(FinalDF$Cut, levels(FinalDF$Cut)[c(1,2,4,5,3)])
FinalDF$Clarity <- factor(FinalDF$Clarity, levels(FinalDF$Clarity)[c(1,4,5,2,3)])
## Plot of Caret vs Price and Amarder Analysis --------------------------------------------------------
# Plots
# Cut
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Cut)) +
geom_point(aes(shape=Cut, color=Cut)) +
ggtitle("Caret vs Price With Cut As Legend") +
theme_bw()
# Color
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Color)) +
geom_point(aes(shape=Color, color=Color)) +
ggtitle("Caret vs Price With Color As Legend") +
theme_bw()
# Clarity
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Clarity)) +
geom_point(aes(shape=Clarity, color=Clarity)) +
ggtitle("Caret vs Price With Clarity As Legend") +
theme_bw()
## Blue Nile’s buying guide describes how the four C’s (cut, color, clarity, and carat weight) are the most
## important characteristics when buying a diamond. It seems reasonable to model price as a function of those
## four characteristics. Having played around with the data bit, a multiplicative model seems like a good choice.
## I model price as a product of carat weight raised to the power β times multipliers for the cut, color, and
## clarity of the diamond.
##
## Pricei∝caratβi ⋅ cuti ⋅ colori ⋅ clarityi
##
## Taking log’s of both sides allows this model to be estimated using a linear regression
##
## log(pricei)=α+βlog(carati)+δcuti+δcolori+δclarityi+ϵi
# Create dummy var for Cut, Color, Clarity and disregard the CutGood, ColorG, and ClarityVS2 as they are dependent on the other respective variables
tempDF <- as.tibble(cbind(model.matrix( ~ Cut - 1, data=FinalDF), model.matrix( ~ Color - 1, data=FinalDF), model.matrix( ~ Clarity - 1, data=FinalDF)))
FinalDF <- as.tibble(cbind(FinalDF, tempDF))
colnames(FinalDF) <- make.names(colnames(FinalDF))
fString <- paste('log(Price) ~ log(Carat)+', paste(colnames(FinalDF)[-c(1:6, 11, 15, 20)], collapse = '+'), sep = '')
fit <- lm(fString, data=FinalDF)
# Correlation
linDependTerm <- alias(fit)
# Find the coeff of fit and use it to plot fitted line
# https://www.statmethods.net/stats/regression.html for more documentation on Fitting lm
coeff=coefficients(fit)
# PLotting regression line
ggplot(FinalDF, aes(x=Carat,
y=Price,
color=Cut)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
# Adding in the regression forecasts back into df
FinalDF <- cbind(FinalDF, Forecast=exp(predict(fit)))
View(FinalDF)
FinalDF$Residual <- cbind(FinalDF, resid(fit))
View(FinalDF)
require(ggplot2)
require(corrplot)
require(tibble)
# Some basic analysis on diamond prices based on https://github.com/amarder/diamonds/blob/master/diamonds.Rmd
# Get cur dir from source of R Script
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
# Import Data
FinalDF <- read.csv("./Clean_Data/Diamond_Data.csv", sep = rawToChar(as.raw(127)))
# Reorder Factor Levels
FinalDF$Cut <- factor(FinalDF$Cut, levels(FinalDF$Cut)[c(1,2,4,5,3)])
FinalDF$Clarity <- factor(FinalDF$Clarity, levels(FinalDF$Clarity)[c(1,4,5,2,3)])
## Plot of Caret vs Price and Amarder Analysis --------------------------------------------------------
# Plots
# Cut
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Cut)) +
geom_point(aes(shape=Cut, color=Cut)) +
ggtitle("Caret vs Price With Cut As Legend") +
theme_bw()
# Color
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Color)) +
geom_point(aes(shape=Color, color=Color)) +
ggtitle("Caret vs Price With Color As Legend") +
theme_bw()
# Clarity
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Clarity)) +
geom_point(aes(shape=Clarity, color=Clarity)) +
ggtitle("Caret vs Price With Clarity As Legend") +
theme_bw()
## Blue Nile’s buying guide describes how the four C’s (cut, color, clarity, and carat weight) are the most
## important characteristics when buying a diamond. It seems reasonable to model price as a function of those
## four characteristics. Having played around with the data bit, a multiplicative model seems like a good choice.
## I model price as a product of carat weight raised to the power β times multipliers for the cut, color, and
## clarity of the diamond.
##
## Pricei∝caratβi ⋅ cuti ⋅ colori ⋅ clarityi
##
## Taking log’s of both sides allows this model to be estimated using a linear regression
##
## log(pricei)=α+βlog(carati)+δcuti+δcolori+δclarityi+ϵi
# Create dummy var for Cut, Color, Clarity and disregard the CutGood, ColorG, and ClarityVS2 as they are dependent on the other respective variables
tempDF <- as.tibble(cbind(model.matrix( ~ Cut - 1, data=FinalDF), model.matrix( ~ Color - 1, data=FinalDF), model.matrix( ~ Clarity - 1, data=FinalDF)))
FinalDF <- as.tibble(cbind(FinalDF, tempDF))
colnames(FinalDF) <- make.names(colnames(FinalDF))
fString <- paste('log(Price) ~ log(Carat)+', paste(colnames(FinalDF)[-c(1:6, 11, 15, 20)], collapse = '+'), sep = '')
fit <- lm(fString, data=FinalDF)
# Correlation
linDependTerm <- alias(fit)
# Find the coeff of fit and use it to plot fitted line
# https://www.statmethods.net/stats/regression.html for more documentation on Fitting lm
coeff=coefficients(fit)
# PLotting regression line
ggplot(FinalDF, aes(x=Carat,
y=Price,
color=Cut)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
# Adding in the regression forecasts back into df
FinalDF <- cbind(FinalDF, Forecast=exp(predict(fit)))
FinalDF <- cbind(FinalDF, Residual=exp(resid(fit)))
View(FinalDF)
?resid
require(ggplot2)
require(corrplot)
require(tibble)
# Some basic analysis on diamond prices based on https://github.com/amarder/diamonds/blob/master/diamonds.Rmd
# Get cur dir from source of R Script
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
# Import Data
FinalDF <- read.csv("./Clean_Data/Diamond_Data.csv", sep = rawToChar(as.raw(127)))
# Reorder Factor Levels
FinalDF$Cut <- factor(FinalDF$Cut, levels(FinalDF$Cut)[c(1,2,4,5,3)])
FinalDF$Clarity <- factor(FinalDF$Clarity, levels(FinalDF$Clarity)[c(1,4,5,2,3)])
## Plot of Caret vs Price and Amarder Analysis --------------------------------------------------------
# Plots
# Cut
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Cut)) +
geom_point(aes(shape=Cut, color=Cut)) +
ggtitle("Caret vs Price With Cut As Legend") +
theme_bw()
# Color
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Color)) +
geom_point(aes(shape=Color, color=Color)) +
ggtitle("Caret vs Price With Color As Legend") +
theme_bw()
# Clarity
ggplot(FinalDF,
aes(x=Carat,
y=Price,
color=Clarity)) +
geom_point(aes(shape=Clarity, color=Clarity)) +
ggtitle("Caret vs Price With Clarity As Legend") +
theme_bw()
## Blue Nile’s buying guide describes how the four C’s (cut, color, clarity, and carat weight) are the most
## important characteristics when buying a diamond. It seems reasonable to model price as a function of those
## four characteristics. Having played around with the data bit, a multiplicative model seems like a good choice.
## I model price as a product of carat weight raised to the power β times multipliers for the cut, color, and
## clarity of the diamond.
##
## Pricei∝caratβi ⋅ cuti ⋅ colori ⋅ clarityi
##
## Taking log’s of both sides allows this model to be estimated using a linear regression
##
## log(pricei)=α+βlog(carati)+δcuti+δcolori+δclarityi+ϵi
# Create dummy var for Cut, Color, Clarity and disregard the CutGood, ColorG, and ClarityVS2 as they are dependent on the other respective variables
tempDF <- as.tibble(cbind(model.matrix( ~ Cut - 1, data=FinalDF), model.matrix( ~ Color - 1, data=FinalDF), model.matrix( ~ Clarity - 1, data=FinalDF)))
FinalDF <- as.tibble(cbind(FinalDF, tempDF))
colnames(FinalDF) <- make.names(colnames(FinalDF))
fString <- paste('log(Price) ~ log(Carat)+', paste(colnames(FinalDF)[-c(1:6, 11, 15, 20)], collapse = '+'), sep = '')
fit <- lm(fString, data=FinalDF)
# Correlation
linDependTerm <- alias(fit)
# Find the coeff of fit and use it to plot fitted line
# https://www.statmethods.net/stats/regression.html for more documentation on Fitting lm
coeff=coefficients(fit)
# PLotting regression line
ggplot(FinalDF, aes(x=Carat,
y=Price,
color=Cut)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
# Adding in the regression forecasts back into df
FinalDF <- cbind(FinalDF, Forecast=exp(predict(fit)))
FinalDF <- cbind(FinalDF, Residual=resid(fit))
View(FinalDF)
View(FinalDF)
FinalDF[227,]$urlList
